#!/usr/bin/env python
# noinspection PyUnresolvedReferences

import datetime
import os
import shutil
import sys
from optparse import OptionParser, SUPPRESS_HELP
from os.path import isfile, join, isdir

from pybedtools import BedTool

import GeneAnnotation
import Utils.reference_data as ref
import targqc
import targqc.config as cfg
from Utils import logger
from Utils.bed_utils import verify_bed
from Utils.bam_utils import verify_bam
from Utils.file_utils import adjust_path, safe_mkdir, verify_file, remove_quotes, file_exists, which
from Utils.logger import critical, err, info, warn, debug, is_local
from Utils.proc_args import read_samples, find_bams, find_fastq_pairs
from Utils.sambamba import index_bam
from Utils import sambamba
from targqc import Sample
from targqc.Target import Target
from targqc.fastq import align, downsample, count_records_in_fastq
from targqc.parallel import ParallelCfg, parallel_view

from targqc.general_report import make_general_reports
from targqc.region_coverage import make_region_reports
from targqc.summarize import summarize_targqc

options = [
    (['--test'], dict(
        dest='test',
        help='Quick test of correct installation.'
    )),
    (['--bed', '--capture', '--amplicons'], dict(
        dest='bed',
        help='BED file with regions to analyse. If not specified, CDS across full genome will be analysed',
    )),
    (['-o', '--output-dir'], dict(
        dest='output_dir',
        metavar='DIR',
        help='Output directory (or directory name in case of bcbio final dir)',
        default=os.getcwd(),
     )),
    (['-g', '--genome'], dict(
        dest='genome',
        help='Genome build (hg19 or hg38), used to pick genome annotation BED file if not specified',
    )),
    (['--bwa-prefix'], dict(
        dest='bwa_prefix',
        help='Path to BWA index prefix to align if input is FastQ',
     )),
    (['--padding'], dict(
        dest='padding',
        type='int',
        help='integer indicating the number of bases to extend each target region up and down-stream. '
             'Default is ' + str(cfg.padding),
        default=cfg.padding
     )),
    (['--downsample-pairs-num', '--downsample-to'], dict(
        dest='downsample_pairs_num',
        type='int',
        help='If input is FastQ, select N random read pairs from each input set. '
             'Default is ' + str(cfg.downsample_pairs_num) + '. To turn off (align all reads), set --downsample-pairs-num off',
        default=cfg.downsample_pairs_num,
     )),
    (['-t', '--nt', '--threads'], dict(
        dest='threads',
        type='int',
        help='Number of threads'
     )),
    (['--reuse'], dict(
        dest='reuse_intermediate',
        help='reuse intermediate non-empty files in the work dir from previous run',
        action='store_true',
        default=cfg.reuse_intermediate
     )),
    (['-s', '--scheduler'], dict(
        dest='scheduler',
        choices=["lsf", "sge", "torque", "slurm", "pbspro"],
        help="Scheduler to use for ipython parallel"
     )),
    (["-q", "--queue"], dict(
        dest='queue',
        help="Scheduler queue to run jobs on, for ipython parallel"
     )),
    (["-r", "--resources"], dict(
        dest='resources',
        help=("Cluster specific resources specifications. "
          "Can be specified multiple times.\n"
          "Supports SGE, Torque, LSF and SLURM "
          "parameters."),
        default=[],
        action="append")),

    ##############
    ## Extended: #
    (['--bam'], dict(dest='bam', help=SUPPRESS_HELP,)),  # help='path to the BAM file to analyse',)),
    (['-1'], dict(dest='l_fpath', help=SUPPRESS_HELP,)),  # help='fastq left reads, optional instead of BAM')),
    (['-2'], dict(dest='r_fpath', help=SUPPRESS_HELP,)),  # help='fastq right reads, optional instead of BAM')),
    (['--sample', '--name'], dict(dest='sample', help=SUPPRESS_HELP,)),  # help='Sample name (default is part of name of the first parameter prior to the first - or .',)),
    (['--original-bed'], dict(
        dest='original_target_bed',
        help=SUPPRESS_HELP,
     )),
    (['--fai'], dict(
        dest='fai',
        help=SUPPRESS_HELP,  # Path to FAI file - to sort BAM and BED files, and to get chromosome lengths for proper padding BED files; optional
     )),
    # (['--reannotate'], dict(
    #     dest='reannotate',
    #     help='re-annotate BED file with gene names',
    #     action='store_true',
    #     default=True)
    #  ),
    (['--no-prep-bed'], dict(
        dest='prep_bed',
        action='store_false',
        default=True,
        help=SUPPRESS_HELP,
     )),
    (['-e', '--extended'], dict(
        dest='extended',
        action='store_true',
        default=False,
        help=SUPPRESS_HELP,  # 'extended - flagged regions and missed variants',
     )),
    (['--no-dedup'], dict(
        dest='no_dedup',
        action='store_true',
        default=not cfg.dedup,
        help=SUPPRESS_HELP,
     )),
    (['--debug'], dict(
        dest='debug',
        action='store_true',
        default=cfg.debug,
        help=SUPPRESS_HELP,
     )),
    (['--work-dir'], dict(dest='work_dir', metavar='DIR', help=SUPPRESS_HELP)),
    (['--log-dir'], dict(dest='log_dir', metavar='DIR', help=SUPPRESS_HELP)),
    (['--project-name'], dict(dest='project_name', help=SUPPRESS_HELP)),
    (['--email'], dict(dest='email', help=SUPPRESS_HELP)),
]


def _prep_samples(fastqs_by_sample, bam_by_sample, output_dir, work_dir):
    samples = []
    for sname, (l, r) in fastqs_by_sample.items():
        s = Sample(sname, join(output_dir, sname), join(work_dir, sname))
        s.l_fpath = l
        s.r_fpath = r
        samples.append(s)
    for sname, bam_fpath in bam_by_sample.items():
        s = Sample(sname, join(output_dir, sname), join(work_dir, sname), bam=bam_fpath)
        samples.append(s)
    samples.sort(key=lambda _s: _s.key_to_sort())
    for s in samples:
        safe_mkdir(s.work_dir)
        safe_mkdir(s.dirpath)
    return samples


# def _prep_beds(prep_bed, work_dir, target_bed):
#     target = None
#
#     if prep_bed:
#         info()
#         debug('*' * 70)
#         info('Preparing input BED file: sort, clean, cut, annotate')
#         cfg.features_bed_fpath, target_bed, seq2c_bed = prepare_beds(
#             work_dir, cfg.fai_fpath, cfg.features_bed_fpath, target_bed, cfg.cds_bed_fpath, reuse=cfg.reuse_intermediate)
#
#         if target_bed:
#             target, cfg.features_bed_fpath = extract_gene_names_and_filter_exons(
#                 work_dir, target_bed, cfg.features_bed_fpath, reuse=cfg.reuse_intermediate)
#     else:
#         info('The BED file is ready, skipping preparing.')
#         if target_bed:
#             target, _, _ = extract_gene_names_and_filter_exons(
#                 work_dir, target_bed, cfg.features_bed_fpath, reuse=cfg.reuse_intermediate)
#     info('*' * 70)
#     return target


def main(args):
    output_dir, work_dir, bam_by_sample, fastqs_by_sample, target_bed_fpath = process_opts()

    samples = _prep_samples(fastqs_by_sample, bam_by_sample, output_dir, work_dir)

    target = Target(work_dir, cfg.fai_fpath, cfg.reuse_intermediate, target_bed_fpath)

    info()
    start_targqc(work_dir, samples, target)

    info()
    info('*' * 70)
    info('Summarizing')
    summarize_targqc(cfg.parallel_cfg.threads, output_dir, work_dir, samples, bed_fpath=target_bed_fpath)

    # info()
    # info('Summarizing: running MultiQC')
    # cmd = 'multiqc ' + output_dir + ('' if cfg.reuse_intermediate else ' --force') + ' -v ' + ' '.join(s.dirpath for s in samples)
    # run(cmd)

    if not cfg.debug and work_dir and isdir(work_dir):
        shutil.rmtree(work_dir)


def start_targqc(work_dir, samples, target):
    fastq_samples = [s for s in samples if not s.bam and s.l_fpath and s.r_fpath]
    num_reads_by_sample = dict()
    if fastq_samples:
        with parallel_view(len(fastq_samples), cfg.parallel_cfg) as view:
            num_reads_by_sample = proc_fastq(fastq_samples, view, work_dir, cfg.bwa_prefix,
                                             cfg.downsample_pairs_num, cfg.dedup)

    for s in samples:
        info(s.name + ': using alignment ' + s.bam)

    info()
    with parallel_view(len(samples), cfg.parallel_cfg) as view:
        info('Indexing BAMs...')
        view.run(index_bam, [[s.bam] for s in samples])

        info('Making general reports...')
        make_general_reports(view, samples, target, num_reads_by_sample)

        info()
        info('Making region-level reports...')
        make_region_reports(view, work_dir, samples, target)

    # for general_report, per_gene_report, sample in zip(general_reports, per_gene_reports, samples):
    #     info('')
    #     info('*' * 70)
    #     if general_report.txt_fpath and verify_file(general_report.txt_fpath):
    #         info('Summary report: ' + general_report.txt_fpath)
    #     if per_gene_report:
    #         path = per_gene_report if isinstance(per_gene_report, basestring) else per_gene_report.txt_fpath
    #         if path and verify_file(path):
    #             info('Region-based report: ' + path)


def set_up_dirs(output_dir=None, work_dir=None, log_dir=None):
    """ Creates output_dir, work_dir; sets up log
    """
    output_dir = adjust_path(output_dir or join(os.getcwd(), targqc.targqc_name))
    safe_mkdir(output_dir, 'output_dir')
    debug('Saving results into ' + output_dir)

    if not work_dir:
        work_dir_name = 'work'
        work_dir = join(output_dir, work_dir_name)
    safe_mkdir(work_dir, 'working directory')
    cfg.work_dir = work_dir

    set_up_log(log_dir or work_dir)

    return output_dir, work_dir


def set_up_log(log_dir):
    log_fname = targqc.targqc_name + '.log'
    log_fpath = join(log_dir, log_fname)

    if file_exists(log_fpath):
        timestamp = datetime.datetime.fromtimestamp(os.stat(log_fpath).st_mtime)
        mv_log_fpath = log_fpath + '.' + timestamp.strftime("%Y-%m-%d_%H-%M-%S")
        try:
            if isfile(mv_log_fpath):
                os.remove(mv_log_fpath)
            if not isfile(mv_log_fpath):
                os.rename(log_fpath, mv_log_fpath)
        except OSError:
            pass

    logger.log_fpath = log_fpath
    debug('Logging to ' + log_fpath)
    debug()


def process_opts():
    parser = OptionParser(description='Target coverage evaluation tool.')
    parser.set_usage('Usage: %prog *.bam -o targqc_stats [--bed target.bed ...]')
    for args, kwargs in options:
        parser.add_option(*args, **kwargs)
    opts, args = parser.parse_args()

    cfg.debug = logger.is_debug = opts.debug

    output_dir, work_dir = set_up_dirs(opts.output_dir, opts.work_dir, opts.log_dir)
    debug(' '.join(sys.argv))
    debug()

    tag = ('targqc_' + opts.project_name) if opts.project_name else 'targqc'
    cfg.parallel_cfg = ParallelCfg(opts.scheduler, opts.queue, opts.resources, opts.threads, tag)

    cfg.reuse_intermediate = opts.reuse_intermediate

    bam_by_sample = dict()
    fastqs_by_sample = dict()

    if opts.bam or opts.l_fpath:
        if opts.sample_name:
            sample_name = remove_quotes(opts.sample_name)
            if opts.bam:
                bam_by_sample[sample_name] = verify_bam(opts.bam, is_critical=True)
            elif opts.l_fpath:
                l_fpath = verify_bam(opts.l_fpath, is_critical=True)
                r_fpath = verify_bam(opts.l_fpath)
                fastqs_by_sample[sample_name] = l_fpath, r_fpath
        else:
            if opts.bam:
                bam_by_sample = find_bams(opts.bam)
            elif opts.l_fpath:
                fastqs_by_sample = find_fastq_pairs([opts.l_fpath, opts.r_fpath])
    else:
        fastqs_by_sample, bam_by_sample = read_samples(args)

    if fastqs_by_sample:
        # if not opts.bwa_prefix:
        #     critical('--bwa-prefix is required when running from fastq')
        cfg.bwa_prefix = adjust_path(opts.bwa_prefix)

    if opts.downsample_pairs_num == 'off':
        cfg.downsample_pairs_num = None
    else:
        cfg.downsample_pairs_num = opts.downsample_pairs_num
    cfg.padding = opts.padding
    cfg.genome = opts.genome
    cfg.dedup = not opts.no_dedup

    cfg.original_target_bed = opts.original_target_bed

    bed_fpath = None
    if opts.bed:
        bed_fpath = verify_bed(opts.bed, is_critical=True)
        info('Using target BED file ' + bed_fpath)
    else:
        info('No input BED. Assuming whole genome. For region-based reports, analysing RefSeq CDS.')

    if opts.fai:
        cfg.fai_fpath = opts.fai
    elif cfg.genome:
        cfg.fai_fpath = ref.get_fai(cfg.genome)
    else:
        err('Cannot analyse BED files without --fai or --genome specified')

    return output_dir, work_dir, bam_by_sample, fastqs_by_sample, bed_fpath


def count_reads(s_name, work_dir, fastq_fpath, reuse=False):
    from os.path import join, isfile
    from targqc.fastq import count_records_in_fastq
    from Utils.file_utils import verify_file
    from Utils.logger import info

    read_counts_fpath = join(work_dir, 'original_read_count.txt')
    if reuse and isfile(read_counts_fpath) and verify_file(read_counts_fpath, cmp_date_fpath=fastq_fpath):
        with open(read_counts_fpath) as f:
            return int(f.read().strip())
    else:
        info('Counting read numbers in ' + s_name + ', writing to ' + read_counts_fpath)
        pairs_number = count_records_in_fastq(fastq_fpath)
        read_number = pairs_number * 2
        with open(read_counts_fpath, 'w') as out:
            out.write(str(read_number))
        return read_number


def proc_fastq(samples, parall_view, work_dir, bwa_prefix, downsample_pairs_num, dedup=True):
    num_reads_by_sample = dict()
    if downsample_pairs_num:
        info('Counting read numbers')
        read_counts = parall_view.run(count_reads, [[s.name, s.work_dir, s.l_fpath, cfg.reuse_intermediate] for s in samples])
        for s, read_count in zip(samples, read_counts):
            num_reads_by_sample[s.name] = read_count

        info('Downsampling the reads to ' + str(int(downsample_pairs_num)))
        fastq_pairs = parall_view.run(downsample,
            [[s.work_dir, s.name, s.work_dir, s.l_fpath, s.r_fpath, downsample_pairs_num, 'subset']
             for s in samples])
        for s, (l_r, r_r) in zip(samples, fastq_pairs):
            s.l_fpath = l_r
            s.r_fpath = r_r

    bwa = which('bwa')
    samblaster = which('samblaster')
    samtools = which('samtools')
    sb = sambamba.get_executable()
    if not (bwa and samblaster and samtools and sb):
        if not bwa:         err('Error: bwa is required for alignment')
        if not samblaster:  err('Error: samblaster is required for deduplication')
        if not samtools:    err('Error: samtools is required for alignment')
        if not sb:          err('Error: sambamba is required')
        critical()
    info()
    info('Aligning reads to the reference')
    bam_fpaths = parall_view.run(align,
        [[s.work_dir, s.name, s.l_fpath, s.r_fpath, bwa, samblaster, samtools, sb, bwa_prefix, dedup,
            parall_view.cores_per_job]
         for s in samples])

    bam_fpaths = map(verify_bam, bam_fpaths)
    if len(bam_fpaths) < len(samples):
        critical('Some samples were not aligned successfully.')
    for bam, s in zip(bam_fpaths, samples):
        s.bam = bam

    return num_reads_by_sample


if __name__ == '__main__':
    main(sys.argv)


